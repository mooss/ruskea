#+TITLE: Distanciel modèles de Markov cachés
#+AUTHOR: Félix Jamet
#+OPTIONS: toc:2
#+LANGUAGE: fr
#+PROPERTY: header-args:ipython :session markexec :results silent :tangle markov.py :eval no-export :noweb yes

* Consignes

L’expérimentation présentée dans l’article est (à mon avis) passionnante. Et il serait intéressant de la reproduire sur une autre langue, par exemple la langue française. Pour cela vous devrez:

 - Trouver un corpus en langue française, de taille raisonnable (prendre en référence ce qui est proposé dans l’article)
 - Nettoyer ce corpus pour ne garder que les 26 lettres de l’alphabet et les espaces
 - Utiliser un EM/Baum Welch déjà implémenté (par exemple dans les bibliothèques des langages de programmation) ou utilisez le pseudo-code fourni dans l’algorithme pour réimplémenter votre Baum Welch, pour apprendre les paramètres de votre HMM.
 - Dessinez le HMM (si vous avez utilisé une bibliothèque) et analysez les résultats : à deux classes a-t-on bien les voyelles et les consonnes?

Si vous êtes plus de 2 à faire le choix 4, il est demandé de regarder d’autres langues, en particulier l’Espagnol et l’Allemand. On peut prendre comme base : n étudiants : n-1 langues.

* Quelques définitions
 - États :: ce que l'on cherche à prédir.
 - Observations :: informations supplémentaires que l'on va utiliser afin de prédire les états.

* Notations

#+CAPTION: Notation des modèles de Markov
#+NAME: tbl.notations
| symbole                                                                  | signification                        |
|--------------------------------------------------------------------------+--------------------------------------|
| $A$                                                                      | matrice des transitions              |
| $B$                                                                      | matrice des observations             |
| $\pi$                                                                    | distribution initiale des états      |
|--------------------------------------------------------------------------+--------------------------------------|
| $N$                                                                      | nombre d'états dans le modèle        |
| $Q = \{q_0, q_1, \dots, \q_{N-1}\}$                                      | ensemble des états                   |
|--------------------------------------------------------------------------+--------------------------------------|
| $M$                                                                      | nombres de symboles d'observation    |
| $V = \{0, 1, \dots, M-1\}$                                               | ensemble des observations possibles  |
| $T$                                                                      | longueur de la chaine d'observations |
| $\mathcal{O} = (\mathcal{O}_0, \mathcal{O}_1, \dots, \mathcal{O}_{T-1})$ | chaine d'observations                |

La table [[tbl.notations]] est séparée en trois parties.
La première rassemble ce qui définit un modèle de Markov, la deuxième est constituée de caractéristiques calculées et la dernière partie concerne les observations.

La matrice des transitions est notée $A = \{a_{i,j}\}$, avec
$a_{i,j} = P(\text{ état } q_j \text{ au temps } t+1 | \text{ état } q_i \text{ au temps } t)$.
Ainsi, si on envisage de manipuler la matrice $A$ comme un tableau de tableaux, on a $A[i][j] = a_{i,j}$


$A_{i,j}$ correspond à la probabilité d'être dans l'état $q_j$ sachant qu'on était avant dans l'état $q_i$.
Autrement dit, la probabilité de passer dans l'état $q_j$ si l'on est dans l'état $q_i$.
On remarque que les probabilités des transitions sont indépendantes du temps $t$.

La matrice des observations est notée $B = \{b_j(k)\}$, avec
$$b_j(k) = P(\text{observation } k \text{ au temps } t | \text{ état } q_j \text{ au temps } t)$$
$b_j(k)$ est donc la probabilité d'observer $k$ en étant dans l'état $q_j$. Bien que surprenante, la notation $b_j(k)$ semble être standard dans le domaine des modèles de Markov.

$\pi$ est la distribution initiale des états, c'est à dire la probabilité de démarer dans chacun des état. Il s'agit donc là encore d'une matrice stochastique.

Un modèle de Markov caché (MMC) est défini par $A$, $B$ et $\pi$, et se note typiquement $$\lambda = (A, B, \pi)$$

* Problèmes pour lesquels les MMC sont utiles
Il existe trois problèmes particuliers qui peuvent être résolus à l'aide des modèles de Markov cachés.

** Problème 1
Étant donné un MMC et une chaine d'observations, trouver la probabilité de cette chaine selon ce modèle. Autrement dit, étant donné le MMC $\lambda = (A, B, \pi)$ et la chaine d'observation 
$\mathcal{O} = (\mathcal{O}_0, \mathcal{O}_1, \dots, \mathcal{O}_{T-1})$
, trouver $P(\mathcal{O} | \lambda )$.

Cette probabilité correspond à la somme des probabilités d'observer $\mathcal{O}$ sur tous les arrangements avec répétition de longueur $T$ des états de $\lambda$.
Étant donné que cette méthode revient à faire une somme sur $N^T$ éléments, on développe l'intuition qu'elle n'est pas viable.

** Problème 2
Étant donné un MMC et une chaine d'observation, trouver l'enchainement d'états optimal correspondant.

Les enchainements optimaux d'états trouvés par la programmation dynamique et par les modèles de Markov cachés sont susceptibles de différer. En effet, la programmation dynamique permettra de trouver l'enchainement d'états ayant la plus haute probabilité, tandis que les MMC vont trouver l'enchainement dont les états ont la plus grande probabilité d'être individuelement corrects.
Autrement dit, les MMC vont permettre de maximiser le nombre d'états corrects.

** Problème 3
Étant donné une chaine d'observation, un nombre d'états et un nombre de symboles, trouver le MMC maximisant la probabilité de cette chaine d'observation, autrement dit, entrainer un HMM pour le faire correspondre aux observations.

* Réimplémentation de Baum-Welch
:PROPERTIES:
:header-args:ipython: :session markexec :results silent :tangle markov.py
:END:
** Modèles de Markov
 
#+BEGIN_SRC ipython :results silent
  import math
  import random
  import numpy as np
  from copy import deepcopy

  def stochastic_variation(mat, epsilon):
      """Slightly changes the values of a matrix while making sure that the sum of the rows are kept the same.

      Parameters
      ----------
      mat : np.matrix
          Matrix to change.

      epsilon : float
          Maximal variation.
      """
      random.seed()
      for row in mat:
          delta = 0
          for i in range(0, len(row)):
              # if delta > epsilon / 2:
              #     nextvariation = random.uniform(-epsilon, 0)
              # elif delta < -epsilon / 2:
              #     nextvariation = random.uniform(0, epsilon)
              # else:
              #     nextvariation = random.uniform(-epsilon, epsilon)

              # delta += nextvariation
              nextvalue = random.gauss(row[i], epsilon)
              delta += nextvalue - row[i]
              row[i] = nextvalue
          meandelta = delta/len(row)
          for i in range(0, len(row)):
              row[i] -= meandelta
    

  class markovmodel(object):
      def fromscratch(N, M):
          """Create a Markov model from scratch with the following matrices dimensions:
           - A is NxN
           - B is NxM
           - PI is 1xN

          Parameters
          ----------
          N : int

          M : int

          Returns
          -------
          out : The corresponding Markov model
          """
          inverseN = 1 / N
          inverseM = 1 / M

          transition = np.full((N, N), inverseN)
          observation = np.full((N, M), inverseM)
          initial = np.full((1, N), inverseN)

          stochastic_variation(transition, inverseN / 10)
          stochastic_variation(observation, inverseM / 10)
          stochastic_variation(initial, inverseN / 10)

          return markovmodel(transition, observation, initial)

      def __init__(self,
                   transition_matrix,
                   observation_matrix,
                   initial_state_distribution,
                   rel_tol=1e-9):
          """Create a markov model.

          Parameters
          ----------
          transition_matrix : np.matrix
              NxN matrix containing the state transitions probabilities.

          observation_matrix : np.matrix
              NxM matrix containing the observation probabilities.

          initial_state_distribution : np.matrix
              1xN matrix containing the initial state distribution
          """
          self.transition_matrix = transition_matrix
          self.observation_matrix = observation_matrix
          self.initial_state_distribution = initial_state_distribution
          self.rel_tol = rel_tol
          self.ensure_dimensional_validity()
          self.ensure_row_stochasticity()

          self.ndim = transition_matrix.shape[0]
          self.mdim = observation_matrix.shape[1]

      def __str__(self):
          return '\n'.join((
              'transition:',
              str(self.transition_matrix), '',
              'observation:',
              str(self.observation_matrix), '',
              'initial states:',
              str(self.initial_state_distribution)))

      def ensure_dimensional_validity(self):
          """Raises an exception if the matrices' dimensions are not right.
          """
          tr_rows, tr_columns = self.transition_matrix.shape
          ob_rows, _ = self.observation_matrix.shape
          in_rows, in_columns = self.initial_state_distribution.shape

          if not (tr_rows == tr_columns == ob_rows == in_columns):
              raise ValueError('The number of transition rows, transition columns, observation rows and initial state distribution columns is not the same')

          if in_rows != 1:
              raise ValueError("The initial state distribution matrix should have one and only one row")

      def ensure_row_stochasticity(self):
          """Raises an exception if the matrices are not row-stochastic.
          """
          def fullofones(iterable):
              return all(math.isclose(el, 1, rel_tol = self.rel_tol) for el in iterable)

          if not fullofones(self.transition_matrix.sum(axis=1)):
              raise ValueError("The transition matrix is not row stochastic")

          if not fullofones(self.observation_matrix.sum(axis=1)):
              raise ValueError("The observation matrix is not row stochastic")

          if not fullofones(self.initial_state_distribution.sum(axis=1)):
              raise ValueError("The initial_state_distribution matrix is not row stochastic")

      def getinitialstate(self, i):
          return self.initial_state_distribution[0,i]
#+END_SRC

*** Tests
:PROPERTIES:
:header-args:ipython: :tangle markov_tests.py :session markexec :results output replace
:END:

**** Initialisation

#+BEGIN_SRC ipython :shebang "#!/usr/bin/env python3" :eval never :exports none
  from markov import *
#+END_SRC

**** Création /from scratch/
#+BEGIN_SRC ipython 
  markovtest = markovmodel.fromscratch(3, 4)
  print(markovtest.transition_matrix)
#+END_SRC

#+RESULTS:
: [[0.33018352 0.33518252 0.33463396]
:  [0.33353123 0.33100466 0.33546411]
:  [0.33013537 0.33643941 0.33342522]]

**** Exemple prédiction de température
Il s'agit ici de tester la création des chaines de markov en utilisant l'exemple de prédiction de température.

#+BEGIN_SRC ipython
  try:
      markovtemperature = markovmodel(
          np.matrix([[0.7, 0.3],
                     [0.4, 0.6]]),
          np.matrix([[0.1, 0.4, 0.5],
                     [0.7, 0.2, 0.1]]),
          np.matrix([[0.6, 0.4]])
      )
      print('transition:', markovtemperature.transition_matrix,
            'observation:', markovtemperature.observation_matrix,
            'initial states:', markovtemperature.initial_state_distribution,
            sep='\n')
  except Exception as e:
      print('construction failed:', str(e))
#+END_SRC

#+RESULTS:
: transition:
: [[0.7 0.3]
:  [0.4 0.6]]
: observation:
: [[0.1 0.4 0.5]
:  [0.7 0.2 0.1]]
: initial states:
: [[0.6 0.4]]

** Forward

#+BEGIN_SRC ipython :results output silent

  def alpha_pass(markov, observations):
      """Implementation of the forward algorithm to compute the alpha_t values.

      Parameters
      ----------
      markov : markovchain

      observations : iterable

      Returns
      -------
      out : np.array
          The alpha_t values.
      """
      alpha = np.zeros(shape=(len(observations), markov.ndim))
      scale_factors = np.zeros(shape=(len(observations)))
    
      # alpha_zero initialization

      for i in range(0, markov.ndim):
          alpha[0, i] = markov.getinitialstate(i) * markov.observation_matrix[i, 0]
          scale_factors[0] += alpha[0, i]

      scale_factors[0] = 1 /scale_factors[0]
    
      for i in range(0, markov.ndim):
          alpha[0, i] *= scale_factors[0]

      # alpha_t computation
      for t in range(1, len(observations)):
          for i in range(0, markov.ndim):
              for j in range(0, markov.ndim):
                  alpha[t, i] += alpha[t - 1, j] * markov.transition_matrix[j, i]
              alpha[t, i] *= markov.observation_matrix[i, observations[t]]
              scale_factors[t] += alpha[t, i]

          # scale alpha
          scale_factors[t] = 1 / scale_factors[t]
          for i in range(0, markov.ndim):
              alpha[t, i] *= scale_factors[t]

      return (alpha, scale_factors)
#+END_SRC

*** Test
:PROPERTIES:
:header-args:ipython: :tangle markov_tests.py :session markexec :results output replace
:END:
#+BEGIN_SRC ipython
  observations = [0, 1, 0, 2]
  alpha_matrix, scales = alpha_pass(markovtemperature, observations)
  print(alpha_matrix)
  print(scales)
#+END_SRC

#+RESULTS:
: [[0.17647059 0.82352941]
:  [0.62348178 0.37651822]
:  [0.16880093 0.83119907]
:  [0.8039794  0.1960206 ]]
: [2.94117647 3.44129555 2.87543655 3.56816483]

**** backup
#+RESULTS:
: [[0.17647059 0.82352941]
:  [0.62348178 0.37651822]
:  [0.16880093 0.83119907]
:  [0.8039794  0.1960206 ]]

** Backward

#+BEGIN_SRC ipython :results output silent
  def beta_pass(markov, observations, scale_factors):
      """

      Parameters
      ----------
      markov : 

      observations : 

      Returns
      -------
      out : 

      """
      beta = np.zeros(shape=(len(observations), markov.ndim))

      # all elements of the last column take the last scale factor as value
      # np.vectorize(lambda _: scale_factors[-1])(beta.transpose()[-1])
      # for line in beta:
      #     line[-1] = scale_factors[-1]
      for i in range(0, markov.ndim):
          beta[-1, i] = scale_factors[-1]

      for t in reversed(range(0, len(observations) - 1)):
          for i in range(0, markov.ndim):
              for j in range(0, markov.ndim):
                  beta[t, i] += markov.transition_matrix[i, j] * markov.observation_matrix[j, observations[t+1]] * beta[t + 1, j]

              # scale beta
              beta[t, i] *= scale_factors[t]

      return beta
#+END_SRC

*** Tests
:PROPERTIES:
:header-args:ipython: :tangle markov_tests.py :session markexec :results output replace
:END:

#+BEGIN_SRC ipython
  beta_matrix = beta_pass(markovtemperature, observations, scales)
  print(beta_matrix)
#+END_SRC

#+RESULTS:
: [[3.1361635  2.89939354]
:  [2.86699344 4.39229044]
:  [3.898812   2.66760821]
:  [3.56816483 3.56816483]]

** Gamma et di-gamma

#+BEGIN_SRC ipython :results silent
  def gamma_digamma_pass(markov, observations, alpha, beta):
      """

      Parameters
      ----------
      markov : 
    
      observations : 
    
      alpha : 
    
      beta : 
    
      Returns
      -------
      out : 
    
      """
      digamma = np.zeros(shape=(len(observations), markov.ndim, markov.ndim))
      gamma = np.zeros(shape=(len(observations), markov.ndim))

      for t in range(0, len(observations) - 1):
          for i in range(0, markov.ndim):
              for j in range(0, markov.ndim):
                  digamma[t, i, j] = alpha[t, i] * markov.transition_matrix[i, j] * markov.observation_matrix[j, observations[t + 1]] * beta[t + 1, j]
                  gamma[t, i] += digamma[t, i, j]

      # special case for the last gammas
      for i in range(0, markov.ndim - 1):
          gamma[-1, i] = alpha[-1, i]

      return (gamma, digamma)
#+END_SRC

*** Test
:PROPERTIES:
:header-args:ipython: :tangle markov_tests.py :session markexec :results output replace
:END:

#+BEGIN_SRC ipython
  gamma, digamma = gamma_digamma_pass(
      markovtemperature,
      observations,
      alpha_matrix,
      beta_matrix
  )
  print(gamma, '\n\n\n', digamma, sep='')
#+END_SRC

#+RESULTS:
#+begin_example
[[0.18816981 0.81183019]
 [0.51943175 0.48056825]
 [0.22887763 0.77112237]
 [0.8039794  0.        ]]


[[[0.14166321 0.0465066 ]
  [0.37776855 0.43406164]]

 [[0.17015868 0.34927307]
  [0.05871895 0.4218493 ]]

 [[0.21080834 0.01806929]
  [0.59317106 0.17795132]]

 [[0.         0.        ]
  [0.         0.        ]]]
#+end_example


*** =greek_pass=
La fonction =greek_pass= fait office de sucre syntaxique, pour faire toutes les passes définies précédemment en récupérant seulement ce qui nous intéresse, à savoir les gammas et di-gammas.

#+BEGIN_SRC ipython 
  def greek_pass(markov, observations):
      """

      Parameters
      ----------
      markov : 
    
      observations : 
    
      Returns
      -------
      out : 
    
      """
      alpha, scale_factors = alpha_pass(markov, observations)
      beta = beta_pass(markov, observations, scale_factors)
      return (*gamma_digamma_pass(markov, observations, alpha, beta), scale_factors)
#+END_SRC

**** Test
:PROPERTIES:
:header-args:ipython: :tangle markov_tests.py :session markexec :results output replace
:END:

#+BEGIN_SRC ipython
  gamma2, digamma2, scale_factors = greek_pass(markovtemperature, observations)
  if not np.array_equal(gamma, gamma2) or not np.array_equal(digamma, digamma2):
      print('gammas or digammas from greek_pass and from gamma_digamma_pass differ')
  else:
      print('gammas and digammas from greek_pass and from gamma_digamma_pass are the same')

  if not np.array_equal(scales, scale_factors):
      print('the scale factors from alpha_pass et greek_pass differ')
  else:
      print('the scale factors from alpha_pass et greek_pass are the same')
#+END_SRC

#+RESULTS:
: gammas and digammas from greek_pass and from gamma_digamma_pass are the same
: the scale factors from alpha_pass et greek_pass are the same

** Réestimation

*** Distribution initiale des états

#+BEGIN_SRC ipython
  def reestimate_initial_state_distribution(markov, gamma):
      """Use previously-calculated gamma values to do a re-estimation of the initial state distribution.

      Parameters
      ----------
      markov : 
    
      gamma : 
    
      Returns
      -------
      out : 
      """
      for i in range(0, markov.ndim):
          markov.initial_state_distribution[0, i] = gamma[0, i]
#+END_SRC

*** Transitions

#+BEGIN_SRC ipython
  def reestimate_transition_matrix(markov, gamma, digamma):
      """


          Parameters
          ----------
          markov : 

          gamma : 

          digamma : 

          Returns
          -------
          out : 

      """
      for i in range(0, markov.ndim):
          for j in range(0, markov.ndim):
              gamma_acc, digamma_acc = 0, 0
              for t in range(0, len(gamma) - 1):
                  gamma_acc += gamma[t, i]
                  digamma_acc += digamma[t, i, j]
              markov.transition_matrix[i, j] = digamma_acc / gamma_acc

      markov.ensure_row_stochasticity()
#+END_SRC

*** Observations

#+BEGIN_SRC ipython
  def reestimate_observation_matrix(markov, observations, gamma):
      """

      Parameters
      ----------
      markov : 
    
      observations : 
    
      gamma : 
      """
      for i in range(0, markov.ndim):
          for j in range(0, markov.mdim):
              gamma_acc_observed, gamma_acc_all = 0, 0
              for t in range(0, len(observations)):
                  if observations[t] == j:
                      gamma_acc_observed += gamma[t, i]
                  gamma_acc_all += gamma[t, i]
              markov.observation_matrix[i, j] = gamma_acc_observed / gamma_acc_all
#+END_SRC

*** Probabilité de la chaine d'observation
La probabilité de la chaine d'observation selon le modèle de Markov est utilisé pour mesurer l'avancement de l'entrainement de ce modèle.

#+BEGIN_SRC ipython
  def log_observation_sequence_probability(scale_factors):
      """Compute the log of the observation's sequence probability according to a markov model, using the scales factors.

      Parameters
      ----------
      scale_factors : 
    
      Returns
      -------
      out : 
      """
      result = 0
      for i in range(0, len(scale_factors)):
          result += math.log(scale_factors[i])
      return -result
    
#+END_SRC

*** Modèle
On utilise les trois fonctions de réestimation précédentes pour réestimer le modèle dans sa globalité, à partir de la chaine des observations.

#+BEGIN_SRC ipython
  def reestimate_markov_model(markov, observations):
      """

      Parameters
      ----------
      markov : 
    
      observations : 
    
      Returns
      -------
      out : 
      """
      gamma, digamma, scale_factors = greek_pass(markov, observations)
      reestimate_initial_state_distribution(markov, gamma)
      reestimate_transition_matrix(markov, gamma, digamma)
      reestimate_observation_matrix(markov, observations, gamma)
      return log_observation_sequence_probability(scale_factors)
#+END_SRC

*** Boucle de réestimation
L'entrainement d'un modèle de markov se fait en répétant des réevaluations.
On arrête la boucle de réestimation lorsque un nombre pré-déterminé a été achevé ou lorsque la réestimation cesse d'apporter des améliorations par rapport à l'itération précédente.

#+BEGIN_SRC ipython
  def train_markov_model(markov, observations, max_iterations=200, epsilon = 0.000000001):
      """

      Parameters
      ----------
      markov : 

      observations : 

      max_iterations : 

      Returns
      -------
      out : 
      """
      _, scale_factors = alpha_pass(markov, observations)
      bestlogprob = log_observation_sequence_probability(scale_factors)
      bestmodel = deepcopy(markov)

      for i in range(1, max_iterations):
          logprob = reestimate_markov_model(markov, observations)
          markov.ensure_row_stochasticity()
          if logprob > bestlogprob:
              bestmodel = deepcopy(markov)
              bestlogprob = logprob

      markov = deepcopy(bestmodel)
      print('bestprob', bestlogprob)
#+END_SRC

*** Test
:PROPERTIES:
:header-args:ipython: :tangle markov_tests.py :session markexec :results output replace
:END:

#+BEGIN_SRC ipython
  from copy import deepcopy
  markov_copy = deepcopy(markovtemperature)
  print(markov_copy)
  train_markov_model(markov_copy, observations, 10)
  print(markov_copy)
#+END_SRC

#+RESULTS:
#+begin_example
transition:
[[0.7 0.3]
 [0.4 0.6]]

observation:
[[0.1 0.4 0.5]
 [0.7 0.2 0.1]]

initial states:
[[0.6 0.4]]
the model stopped improving at iteration 9
transition:
[[3.80741949e-287 1.00000000e+000]
 [1.00000000e+000 0.00000000e+000]]

observation:
[[9.52278575e-288 5.00000000e-001 5.00000000e-001]
 [1.00000000e+000 0.00000000e+000 0.00000000e+000]]

initial states:
[[1.69480811e-290 1.00000000e+000]]
#+end_example



* Analyse de texte assistée par un modèle de Markov caché

#+BEGIN_SRC ipython
  def map_el_to_int(iterable, alphabet):
      """Map all the elements of an iterable to their index in an alphabet.
      If an element is not in the alphabet, it will be ignored.

      Parameters
      ----------
      iterable : iterable
          The iterable to map.

      alphabet : str
          The letters to keep.

      Returns
      -------
      out : list of int
          The list containing the index of each character in the input string.
      """
      indexation = {letter: index for index, letter in enumerate(alphabet)}
      return (indexation[char] for char in iterable if char in alphabet)

  def markov_alphabetical_analysis(markov, alphabet):
      observation_scores = [[letter,
                             ,*(markov.observation_matrix[state, index]
                                for state in range(0, markov.ndim))]
                            for index, letter in enumerate(alphabet)]

      letter_groups = [list() for _ in range(0, markov.ndim)]
      ungroupables = []

      for letterindex, letter in enumerate(alphabet):
          maxindex = 0
          for state in range(1, markov.ndim):
              if markov.observation_matrix[state, letterindex] >\
                 markov.observation_matrix[maxindex, letterindex]:
                  maxindex = state
              if markov.observation_matrix[maxindex, letterindex] == 0:
                  ungroupables.append(letter)
              else:
                  letter_groups[maxindex].append(letter)

      return observation_scores, letter_groups, ungroupables

#+END_SRC

* noweb
:PROPERTIES:
:header-args:ipython: :tangle no :session none :results silent :eval never
:END:

** corpuses
#+NAME: browncorpus
#+BEGIN_SRC ipython
  with open('brown50000.txt', 'r') as brownfile:
      corpus = brownfile.read().replace('\n', '')
#+END_SRC

** alphabets

#+NAME: latinalphabet
#+BEGIN_SRC ipython
  alphabet = ' abcdefghijklmnopqrstuvwxyz'
#+END_SRC

** Autres

#+NAME: markovimport
#+BEGIN_SRC ipython
  from markov import markovmodel,train_markov_model, map_el_to_int, markov_alphabetical_analysis
#+END_SRC

#+NAME: trainfromscratch
#+BEGIN_SRC ipython
  model = markovmodel.fromscratch(2, len(alphabet))
  print(model)
  train_markov_model(model,
                     list(map_el_to_int(corpus, alphabet)),
                     max_iterations=100)
  table, groups, _ = markov_alphabetical_analysis(model, alphabet)
  print(table)
  print(groups)
#+END_SRC

#+NAME: markov_report
#+BEGIN_SRC ipython
  def latexify(char):
      if char == ' ':
          return '\\textvisiblespace'
      return char


  scoretable, groups, ungroupables = markov_alphabetical_analysis(model, alphabet)
  scoretable = [[latexify(line[0]),
                 ,*('${:.3f}$'.format(probas * 100) for probas in line[1:])]
                for line in scoretable]
  scoretable.insert(0, ['caractère', 'État 1 (%)', 'État 2 (%)'])
  print('#+ATTR_LATEX: :align l l l')
  print(orgmodetable(scoretable, header=True), '\n\n\n')

  groupstable = [['{ ' + ',  '.join((latexify(char) for char in group)) + ' }'
                    for group in groups] ]
  groupstable.insert(0, ['Groupe 1', 'Groupe 2'])

  if len(ungroupables) > 0:
      groupstable[0].insert(
          len(ungroupables), 'Hors groupes')
      groupstable[1].insert(
          len(ungroupables), '{ ' + ', '.join(latexify(char) for char in ungroupables) + ' }')
  print(orgmodetable(groupstable, header=True))
#+END_SRC

* /Brown corpus/                                 :export:
Dans cette section les fonctionnalités offertes par =markov.py= sont utilisées pour reproduire l'expérience de "Marvin le martien" (Stamp 2018)



** Extraction des 50 000 premiers caractères du /Brown corpus/

La première étape est d'extraire les caractères nous intéressant depuis le /Brown corpus/.
En effet, le /Brown corpus/ est distribué avec les annotations intégrées et il faut donc les supprimer.
Les 50000 premiers caractères de ce corpus sont écrits dans le fichier =brown50000.txt=.
#+BEGIN_SRC ipython :session brownextract :results silent :tangle brownextract.py :eval never :shebang "#!/usr/bin/env python3"
  import nltk
  nltk.download('brown')
  nltk.download('nonbreaking_prefixes')
  nltk.download('perluniprops')
  from nltk.corpus import brown
  from nltk.tokenize.moses import MosesDetokenizer

  mdetok = MosesDetokenizer()

  def remove_brown_annotations(sentence):
      return mdetok.detokenize(
          ' '.join(sent).replace('``', '"')\
          .replace("''", '"')\
          .replace('`', "'").split(),
          return_str=True)


  maxnbchar = 50000
  currentnbchar = 0
  charbuffer = []

  alphabet = 'abcdefghijklmnopqrstuvwxyz '

  for sent in brown.sents():
      for char in remove_brown_annotations(sent):
          if currentnbchar < maxnbchar and char in alphabet:
              charbuffer.append(char)
              currentnbchar += 1

  output = 'brown50000.txt'
  with open(output, "w") as text_file:
      text_file.write(''.join(charbuffer))

#+END_SRC

** Imports

#+BEGIN_SRC ipython :shebang "#!/usr/bin/env python3" :eval never :exports code :tangle brownmarvin.py
  import numpy as np
#+END_SRC

** Matrices prédéfinies
:PROPERTIES:
:header-args:ipython: :tangle brownmarvin.py :session markexec :results output replace drawer
:END:

#+BEGIN_SRC ipython :exports code
  from markov import *

  marvin_transition = np.array([[0.47468, 0.52532],
                                [0.51656, 0.48344]])
  marvin_observation = np.array(
      [[0.03688, 0.03735, 0.03408, 0.03455, 0.03828, 0.03782, 0.03922, 0.03688, 0.03408, 0.03875, 0.04062, 0.03735, 0.03968, 0.03548, 0.03735, 0.04062, 0.03595, 0.03641, 0.03408, 0.04062, 0.03548, 0.03922, 0.04062, 0.03455, 0.03595, 0.03408, 0.03408],
       [0.03397, 0.03909, 0.03537, 0.03537, 0.03909, 0.03583, 0.03630, 0.04048, 0.03537, 0.03816, 0.03909, 0.03490, 0.03723, 0.03537, 0.03909, 0.03397, 0.03397, 0.03816, 0.03676, 0.04048, 0.03443, 0.03537, 0.03955, 0.03816, 0.03723, 0.03769, 0.03955]]
  )
  marvin_initial = np.array([[0.51316, 0.48684]])

  with open('brown50000.txt', 'r') as marvinfile:
      marvin_corpus = marvinfile.read().replace('\n', '')

  brown_alphabet = ' abcdefghijklmnopqrstuvwxyz'
  brown_marvin = markovmodel(marvin_transition, marvin_observation, marvin_initial, rel_tol=1e-3)

  print(brown_marvin)
  train_markov_model(brown_marvin,
                     list(map_el_to_int(marvin_corpus, brown_alphabet)),
                     max_iterations=100)

  print(brown_marvin)
#+END_SRC

#+RESULTS:
:RESULTS:
transition:
[[0.47468 0.52532]
 [0.51656 0.48344]]

observation:
[[0.03688 0.03735 0.03408 0.03455 0.03828 0.03782 0.03922 0.03688 0.03408
  0.03875 0.04062 0.03735 0.03968 0.03548 0.03735 0.04062 0.03595 0.03641
  0.03408 0.04062 0.03548 0.03922 0.04062 0.03455 0.03595 0.03408 0.03408]
 [0.03397 0.03909 0.03537 0.03537 0.03909 0.03583 0.0363  0.04048 0.03537
  0.03816 0.03909 0.0349  0.03723 0.03537 0.03909 0.03397 0.03397 0.03816
  0.03676 0.04048 0.03443 0.03537 0.03955 0.03816 0.03723 0.03769 0.03955]]

initial states:
[[0.51316 0.48684]]
bestprob -137446.21950411287
transition:
[[0.23380594 0.76619406]
 [0.70619312 0.29380688]]

observation:
[[2.80663694e-01 1.42545160e-01 2.49692361e-13 6.87847936e-04
  4.35693343e-06 2.25671596e-01 3.04678198e-17 4.44818727e-03
  4.99289965e-04 1.27722118e-01 2.60994980e-29 2.92718516e-03
  6.31027224e-05 4.75214656e-14 1.52968215e-11 1.37685546e-01
  1.85592672e-03 5.64893247e-23 1.02389935e-14 1.51700292e-04
  2.03887624e-02 4.65346022e-02 1.31845386e-29 2.88806231e-18
  1.39398404e-18 8.15092461e-03 2.66239538e-38]
 [6.47312285e-02 1.98900890e-05 2.12158779e-02 5.05608272e-02
  6.67952520e-02 8.33899093e-10 3.43220634e-02 2.29579122e-02
  7.06052946e-02 3.08757982e-12 2.26763913e-03 5.02727812e-03
  7.28137650e-02 3.72430901e-02 1.14842470e-01 2.28922444e-06
  3.28804456e-02 1.53738246e-03 1.00852289e-01 1.07054166e-01
  1.32062759e-01 7.21326070e-07 1.63346886e-02 2.14464853e-02
  4.53527826e-03 1.87378694e-02 1.15303684e-03]]

initial states:
[[1.00000000e+00 2.08732228e-11]]
:END:

#+BEGIN_SRC ipython :tangle no :exports results
  def latexify(char):
      if char == ' ':
          return '\\textvisiblespace'
      return char

  def markov_report(markov, brown_alphabet):
      scoretable, groups, ungroupables = markov_alphabetical_analysis(markov, brown_alphabet)
      scoretable = [[latexify(line[0]),
                     ,*('${:.3f}$'.format(probas * 100) for probas in line[1:])]
                    for line in scoretable]
      scoretable.insert(0, ['caractère', 'État 1 (%)', 'État 2 (%)'])
      print('#+ATTR_LATEX: :align l l l')
      print(orgmodetable(scoretable, header=True), '\n\n\n')

      groupstable = [['{ ' + ',  '.join((latexify(char) for char in group)) + ' }'
                        for group in groups] ]
      groupstable.insert(0, ['Groupe 1', 'Groupe 2'])

      if len(ungroupables) > 0:
          groupstable[0].insert(
              len(ungroupables), 'Hors groupes')
          groupstable[1].insert(
              len(ungroupables), '{ ' + ', '.join(latexify(char) for char in ungroupables) + ' }')
      print(orgmodetable(groupstable, header=True))

  markov_report(brown_marvin, brown_alphabet)
#+END_SRC

#+RESULTS:
:RESULTS:
#+ATTR_LATEX: :align l l l
| caractère         | État 1 (%) | État 2 (%) |
|-------------------|------------|------------|
| \textvisiblespace | $28.066$   | $6.473$    |
| a                 | $14.255$   | $0.002$    |
| b                 | $0.000$    | $2.122$    |
| c                 | $0.069$    | $5.056$    |
| d                 | $0.000$    | $6.680$    |
| e                 | $22.567$   | $0.000$    |
| f                 | $0.000$    | $3.432$    |
| g                 | $0.445$    | $2.296$    |
| h                 | $0.050$    | $7.061$    |
| i                 | $12.772$   | $0.000$    |
| j                 | $0.000$    | $0.227$    |
| k                 | $0.293$    | $0.503$    |
| l                 | $0.006$    | $7.281$    |
| m                 | $0.000$    | $3.724$    |
| n                 | $0.000$    | $11.484$   |
| o                 | $13.769$   | $0.000$    |
| p                 | $0.186$    | $3.288$    |
| q                 | $0.000$    | $0.154$    |
| r                 | $0.000$    | $10.085$   |
| s                 | $0.015$    | $10.705$   |
| t                 | $2.039$    | $13.206$   |
| u                 | $4.653$    | $0.000$    |
| v                 | $0.000$    | $1.633$    |
| w                 | $0.000$    | $2.145$    |
| x                 | $0.000$    | $0.454$    |
| y                 | $0.815$    | $1.874$    |
| z                 | $0.000$    | $0.115$    | 



| Groupe 1                                  | Groupe 2                                                                              |
|-------------------------------------------|---------------------------------------------------------------------------------------|
| { \textvisiblespace,  a,  e,  i,  o,  u } | { b,  c,  d,  f,  g,  h,  j,  k,  l,  m,  n,  p,  q,  r,  s,  t,  v,  w,  x,  y,  z } |
:END:

** Matrices aléatoires
:PROPERTIES:
:header-args:ipython: :session brownrandomexec :results output replace drawer
:END:


#+BEGIN_SRC ipython :exports code :noweb yes :tangle brownrandom.py :shebang "#!/usr/bin/env python3"
  <<markovimport>>

  <<browncorpus>>

  <<latinalphabet>>

  <<trainfromscratch>>

#+END_SRC

#+RESULTS:
:RESULTS:
transition:
[[0.5042994  0.4957006 ]
 [0.46535633 0.53464367]]

observation:
[[0.03058812 0.0337808  0.0369601  0.03919405 0.03062829 0.03641803
  0.04038028 0.03355693 0.03827502 0.03990509 0.03068688 0.03427885
  0.0387205  0.0425566  0.03438756 0.04151139 0.03630166 0.0413063
  0.03727868 0.03852866 0.03967864 0.03107869 0.04034014 0.03815457
  0.04228158 0.0388024  0.03442018]
 [0.03597532 0.03419201 0.03027041 0.03737307 0.03544022 0.0380107
  0.03919648 0.03979279 0.03821221 0.03977158 0.03718207 0.03323748
  0.04055947 0.04042318 0.04095177 0.03726813 0.03592023 0.029822
  0.03382096 0.04019688 0.03821241 0.03778573 0.04141545 0.03279016
  0.04237958 0.03159291 0.03820679]]

initial states:
[[0.46610465 0.53389535]]
bestprob -141839.81470665726
([[' ', 0.16555189761632066, 0.17079257783110852], ['a', 0.06832880441352743, 0.06842778214345126], ['b', 0.012528797805341243, 0.009691743916032882], ['c', 0.028547379703253356, 0.024912933880676887], ['d', 0.026096735407615382, 0.0426075006886916], ['e', 0.1041686651163447, 0.111909217806126], ['f', 0.021057612682942775, 0.014964119462766864], ['g', 0.010455735482342394, 0.017362981306862656], ['h', 0.03757809799243164, 0.03643903395830685], ['i', 0.05885229660182649, 0.06344205061449613], ['j', 0.0009081064156945822, 0.0014262929529604029], ['k', 0.004130391245479934, 0.003920095715495962], ['l', 0.03845607619664694, 0.03743522982774367], ['m', 0.022516726487133588, 0.016539298731269737], ['n', 0.050008178911988756, 0.06859398073054745], ['o', 0.07553766838497544, 0.05743881586411798], ['p', 0.02091468368492243, 0.015360386312954628], ['q', 0.0007323536036915315, 0.0008612874816479736], ['r', 0.057720781677194286, 0.04773422643321494], ['s', 0.05343439349336506, 0.05790569300838245], ['t', 0.08030566119286738, 0.07686613632517347], ['u', 0.02297882235133295, 0.021723730027442472], ['v', 0.008769116172705432, 0.008256422520160416], ['w', 0.012408131432007237, 0.010029730828656542], ['x', 0.0024781808922308543, 0.002253005947771991], ['y', 0.014987897955349494, 0.01245753361956629], ['z', 0.0005468070804601338, 0.0006481920643808612]], [['b', 'c', 'f', 'h', 'k', 'l', 'm', 'o', 'p', 'r', 't', 'u', 'v', 'w', 'x', 'y'], [' ', 'a', 'd', 'e', 'g', 'i', 'j', 'n', 'q', 's', 'z']], [])
:END:


#+BEGIN_SRC ipython :tangle no :exports results :noweb yes
  <<markov_report>>
#+END_SRC

#+RESULTS:
:RESULTS:
#+ATTR_LATEX: :align l l l
| caractère         | État 1 (%) | État 2 (%) |
|-------------------|------------|------------|
| \textvisiblespace | $16.555$   | $17.079$   |
| a                 | $6.833$    | $6.843$    |
| b                 | $1.253$    | $0.969$    |
| c                 | $2.855$    | $2.491$    |
| d                 | $2.610$    | $4.261$    |
| e                 | $10.417$   | $11.191$   |
| f                 | $2.106$    | $1.496$    |
| g                 | $1.046$    | $1.736$    |
| h                 | $3.758$    | $3.644$    |
| i                 | $5.885$    | $6.344$    |
| j                 | $0.091$    | $0.143$    |
| k                 | $0.413$    | $0.392$    |
| l                 | $3.846$    | $3.744$    |
| m                 | $2.252$    | $1.654$    |
| n                 | $5.001$    | $6.859$    |
| o                 | $7.554$    | $5.744$    |
| p                 | $2.091$    | $1.536$    |
| q                 | $0.073$    | $0.086$    |
| r                 | $5.772$    | $4.773$    |
| s                 | $5.343$    | $5.791$    |
| t                 | $8.031$    | $7.687$    |
| u                 | $2.298$    | $2.172$    |
| v                 | $0.877$    | $0.826$    |
| w                 | $1.241$    | $1.003$    |
| x                 | $0.248$    | $0.225$    |
| y                 | $1.499$    | $1.246$    |
| z                 | $0.055$    | $0.065$    | 



| Groupe 1                                                          | Groupe 2                                                      |
|-------------------------------------------------------------------|---------------------------------------------------------------|
| { b,  c,  f,  h,  k,  l,  m,  o,  p,  r,  t,  u,  v,  w,  x,  y } | { \textvisiblespace,  a,  d,  e,  g,  i,  j,  n,  q,  s,  z } |
:END:


* Est-Républicain                                                    :export:
Cette section s'appuie sur un corpus contenant des articles du journal l'Est Républicain, publiés en 1999.
Le corpus est disponible à l'adresse suivante : http://www.cnrtl.fr/corpus/estrepublicain/.

** Extraction du texte
Les articles sont contenus dans des fichiers =XML=. Le script suivant est utilisé pour récupérer le texte des articles en ignorant le balisage.

#+BEGIN_SRC ipython :tangle repextract.py :results silent :eval no-export :shebang "#!/usr/bin/env python3"
  import xml.etree.ElementTree as ET
  from itertools import chain

  root = ET.parse('1999-05-17.xml').getroot()
  articles = root.findall('./tei:text/tei:body/tei:div/tei:div/',
                          {'tei': 'http://www.tei-c.org/ns/1.0'})

  alphabet = ' aàâæbcçdeéèêëfghiîïjklmnoôœpqrstuùûüvwxyÿz'
  # print(list(root))
  # print(articles)

  def filterspaces(iterable):
      prevwasspace = True
      for char in iterable:
          if char == ' ':
              if not prevwasspace:
                  prevwasspace = True
                  yield char
          else:
              yield char
              prevwasspace = False


  charbuffer = (char
                for article in articles
                for paragraph in article.itertext()
                for char in paragraph.lower()
                if char in alphabet)

  with open('1999-05-17.txt', 'w') as output:
      output.write(''.join(filterspaces(charbuffer)))
#+END_SRC

Cette approche a ses limites, par exemple, il y a beaucoup de 'h' isolés à cause de la notation des heures (exemple : de 20h à 20h30). Par ailleurs la suppression de certain caractères spéciaux mène à des juxtapositions non désirables (exemple : saint-mihiel \textrightarrow saintmihiel, l'heure \textrightarrow lheure).

Il serait possible de créer des règles pour traiter ces cas particuliers. Cependant, ils semblent être statistiquement insignifiants, il n'est donc pas important de s'en soucier pour cette expérience.

** Analyse du texte brut
:PROPERTIES:
:header-args:ipython: :tangle reprandom.py :session markexec :results output replace drawer
:END:

#+BEGIN_SRC ipython :shebang "#!/usr/bin/env python3" :eval never :exports none
  from markov import *
#+END_SRC

#+BEGIN_SRC ipython :exports code
  import numpy as np
  from itertools import islice
  with open('1999-05-17.txt', 'r') as repfile:
      repcorpus = repfile.read().replace('\n', '')

  repalphabet = ' aàâæbcçdeéèêëfghiîïjklmnoôœpqrstuùûüvwxyÿz'
  repmarkov = markovmodel.fromscratch(2, len(repalphabet))
  train_markov_model(repmarkov,
                     list(islice(map_el_to_int(repcorpus, repalphabet), 0, 50000)),
                     max_iterations=100)

  print(repmarkov)
#+END_SRC

#+RESULTS:
:RESULTS:
the model stopped improving at iteration 85
transition:
[[0.49721192 0.50278808]
 [0.49819695 0.50180305]]

observation:
[[1.67805670e-01 6.76343963e-02 5.73967675e-03 4.77650863e-04
  0.00000000e+00 8.92741759e-03 2.97674329e-02 3.79860741e-04
  3.55224273e-02 1.18587091e-01 2.26664535e-02 3.02312068e-03
  1.14897570e-03 5.96363655e-05 1.00330039e-02 9.24379772e-03
  1.07551794e-02 5.82810450e-02 2.40733561e-04 0.00000000e+00
  3.11940983e-03 4.19614066e-04 4.91414996e-02 2.26391379e-02
  5.90833701e-02 4.39128805e-02 4.55792601e-04 3.20398840e-04
  2.41390581e-02 5.67033482e-03 5.70517465e-02 6.22524447e-02
  5.62603990e-02 4.49573656e-02 2.20142521e-04 1.78100684e-04
  0.00000000e+00 1.12049340e-02 2.00662626e-04 5.09937487e-03
  2.85940683e-03 0.00000000e+00 5.20356678e-04]
 [1.66802234e-01 6.81645272e-02 5.78025203e-03 4.82337237e-04
  0.00000000e+00 9.03228120e-03 3.00717694e-02 3.80145494e-04
  3.53192041e-02 1.19013317e-01 2.27336893e-02 3.05678573e-03
  1.17094688e-03 6.03615100e-05 1.00869491e-02 9.19660086e-03
  1.06853524e-02 5.82801151e-02 2.39277853e-04 0.00000000e+00
  3.16046479e-03 4.20390714e-04 4.95774865e-02 2.25218460e-02
  5.96352845e-02 4.40473769e-02 4.64178193e-04 3.19611107e-04
  2.41414099e-02 5.21186863e-03 5.73878520e-02 6.19501686e-02
  5.63005338e-02 4.37890222e-02 2.19863119e-04 1.81885586e-04
  0.00000000e+00 1.13944240e-02 1.99347352e-04 5.14053852e-03
  2.86064423e-03 0.00000000e+00 5.19656833e-04]]

initial states:
[[0.64613372 0.35386628]]
:END:
 
#+BEGIN_SRC ipython :exports results :tangle no
  markov_report(repmarkov, repalphabet)
#+END_SRC

#+RESULTS:
:RESULTS:
#+ATTR_LATEX: :align l l l
| caractère         | État 1 (%) | État 2 (%) |
|-------------------|------------|------------|
| \textvisiblespace | $16.781$   | $16.680$   |
| a                 | $6.763$    | $6.816$    |
| à                 | $0.574$    | $0.578$    |
| â                 | $0.048$    | $0.048$    |
| æ                 | $0.000$    | $0.000$    |
| b                 | $0.893$    | $0.903$    |
| c                 | $2.977$    | $3.007$    |
| ç                 | $0.038$    | $0.038$    |
| d                 | $3.552$    | $3.532$    |
| e                 | $11.859$   | $11.901$   |
| é                 | $2.267$    | $2.273$    |
| è                 | $0.302$    | $0.306$    |
| ê                 | $0.115$    | $0.117$    |
| ë                 | $0.006$    | $0.006$    |
| f                 | $1.003$    | $1.009$    |
| g                 | $0.924$    | $0.920$    |
| h                 | $1.076$    | $1.069$    |
| i                 | $5.828$    | $5.828$    |
| î                 | $0.024$    | $0.024$    |
| ï                 | $0.000$    | $0.000$    |
| j                 | $0.312$    | $0.316$    |
| k                 | $0.042$    | $0.042$    |
| l                 | $4.914$    | $4.958$    |
| m                 | $2.264$    | $2.252$    |
| n                 | $5.908$    | $5.964$    |
| o                 | $4.391$    | $4.405$    |
| ô                 | $0.046$    | $0.046$    |
| œ                 | $0.032$    | $0.032$    |
| p                 | $2.414$    | $2.414$    |
| q                 | $0.567$    | $0.521$    |
| r                 | $5.705$    | $5.739$    |
| s                 | $6.225$    | $6.195$    |
| t                 | $5.626$    | $5.630$    |
| u                 | $4.496$    | $4.379$    |
| ù                 | $0.022$    | $0.022$    |
| û                 | $0.018$    | $0.018$    |
| ü                 | $0.000$    | $0.000$    |
| v                 | $1.120$    | $1.139$    |
| w                 | $0.020$    | $0.020$    |
| x                 | $0.510$    | $0.514$    |
| y                 | $0.286$    | $0.286$    |
| ÿ                 | $0.000$    | $0.000$    |
| z                 | $0.052$    | $0.052$    | 



| Groupe 1                                                                  | Groupe 2                                                                                              | Hors groupes   |
|---------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|----------------|
| { \textvisiblespace,  d,  g,  h,  i,  î,  m,  œ,  q,  s,  u,  ù,  w,  z } | { a,  à,  â,  b,  c,  ç,  e,  é,  è,  ê,  ë,  f,  j,  k,  l,  n,  o,  ô,  p,  r,  t,  û,  v,  x,  y } | { æ, ï, ü, ÿ } |
:END:

À première vue, les résultats ne sont pas concluants. Peut-être qu'un linguiste saura interpréter ces résultats, mais il est plus probable que l'utilisation d'un grand nombre de caractères peu fréquents perturbe l'entrainement du modèle.

** Analyse sans accents et ligatures
:PROPERTIES:
:header-args:ipython: :tangle reprandom_noaccent.py :session markexec :results output replace drawer
:END:

#+BEGIN_SRC ipython :shebang "#!/usr/bin/env python3" :eval never :exports none
  from markov import *
#+END_SRC

#+BEGIN_SRC ipython :exports code
    import numpy as np
    from itertools import islice
    with open('1999-05-17.txt', 'r') as repfile:
        repcorpus = repfile.read().replace('\n', '')

    def translate(iterable, translations):
        for el in iterable:
            if el in translations:
                for tr in translations[el]:
                    yield tr
            else:
                yield el

    repalphabet_latin = ' abcdefghijklmnopqrstuvwxyz'
    translations = {'à': 'a',
                    'â': 'a',
                    'æ': 'ae',
                    'ç': 'c',
                    'é': 'e',
                    'è': 'e',
                    'ê': 'e',
                    'ë': 'e',
                    'î': 'i',
                    'ï': 'i',
                    'ô': 'o',
                    'œ': 'oe',
                    'ù': 'u',
                    'û': 'u',
                    'ü': 'u',
                    'ÿ': 'y',
                    '\'': ' ',
                    '-': ' '}

    #' aàâæbcçdeéèêëfghiîïjklmnoôœpqrstuùûüvwxyÿz'
    observations = list(islice(
        map_el_to_int(translate(repcorpus, translations), repalphabet_latin),
        0, 50000))
    repmarkov_latin = markovmodel.fromscratch(2, len(repalphabet_latin))

    train_markov_model(repmarkov_latin, observations, max_iterations=100)

    print(repmarkov_latin)
#+END_SRC

#+RESULTS:
:RESULTS:
the model never stopped improving
transition:
[[0.49791517 0.50208483]
 [0.49957854 0.50042146]]

observation:
[[0.16803065 0.07352529 0.00897934 0.03117499 0.03533547 0.14544868
  0.01001141 0.00917099 0.01077808 0.05838684 0.00310831 0.00042099
  0.04936585 0.0225495  0.05936101 0.04460847 0.02391233 0.00595804
  0.05706367 0.06223613 0.05604983 0.04457401 0.01127332 0.00019916
  0.00512855 0.00283373 0.00051535]
 [0.16641661 0.0746733  0.00898083 0.02941051 0.0355048  0.14671103
  0.01006864 0.00926894 0.01066242 0.05861373 0.0031716  0.00041902
  0.04931523 0.02261079 0.05932025 0.04487175 0.02436701 0.00492463
  0.05733677 0.06192585 0.05647022 0.04494594 0.01132677 0.00020084
  0.00511159 0.00284629 0.00052464]]

initial states:
[[0.75211754 0.24788246]]
:END:

 
#+BEGIN_SRC ipython :exports results :tangle no
  markov_report(repmarkov_latin, repalphabet_latin)
#+END_SRC

#+RESULTS:
:RESULTS:
#+ATTR_LATEX: :align l l l
| caractère         | État 1 (%) | État 2 (%) |
|-------------------|------------|------------|
| \textvisiblespace | $16.803$   | $16.642$   |
| a                 | $7.353$    | $7.467$    |
| b                 | $0.898$    | $0.898$    |
| c                 | $3.117$    | $2.941$    |
| d                 | $3.534$    | $3.550$    |
| e                 | $14.545$   | $14.671$   |
| f                 | $1.001$    | $1.007$    |
| g                 | $0.917$    | $0.927$    |
| h                 | $1.078$    | $1.066$    |
| i                 | $5.839$    | $5.861$    |
| j                 | $0.311$    | $0.317$    |
| k                 | $0.042$    | $0.042$    |
| l                 | $4.937$    | $4.932$    |
| m                 | $2.255$    | $2.261$    |
| n                 | $5.936$    | $5.932$    |
| o                 | $4.461$    | $4.487$    |
| p                 | $2.391$    | $2.437$    |
| q                 | $0.596$    | $0.492$    |
| r                 | $5.706$    | $5.734$    |
| s                 | $6.224$    | $6.193$    |
| t                 | $5.605$    | $5.647$    |
| u                 | $4.457$    | $4.495$    |
| v                 | $1.127$    | $1.133$    |
| w                 | $0.020$    | $0.020$    |
| x                 | $0.513$    | $0.511$    |
| y                 | $0.283$    | $0.285$    |
| z                 | $0.052$    | $0.052$    | 



| Groupe 1                                              | Groupe 2                                                                  |
|-------------------------------------------------------|---------------------------------------------------------------------------|
| { \textvisiblespace,  c,  h,  k,  l,  n,  q,  s,  x } | { a,  b,  d,  e,  f,  g,  i,  j,  m,  o,  p,  r,  t,  u,  v,  w,  y,  z } |
:END:



* Sources
Stamp, Mark. (2018). A Revealing Introduction to Hidden Markov Models. https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf.

* 100 iter on brown backup
#+RESULTS:
#+begin_example
27 [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
50000 out of 50000
1.0000299999999998
1.00003
the model never stopped improving
CPU times: user 1min 59s, sys: 161 ms, total: 1min 59s
Wall time: 1min 59s
transition:
[[0.23368789 0.76631211]
 [0.70597863 0.29402137]]

observation:
[[2.80687985e-01 1.42581906e-01 1.40629293e-13 6.66524349e-04
  3.56911264e-06 2.25725373e-01 1.37666921e-17 4.42155954e-03
  4.87498731e-04 1.27752553e-01 6.73394223e-30 2.92496160e-03
  5.50924144e-05 2.54038203e-14 9.00794057e-12 1.37718752e-01
  1.83313919e-03 1.75675908e-23 5.22460779e-15 1.36925065e-04
  2.03105974e-02 4.65458472e-02 3.21539099e-30 1.30524222e-18
  6.21332760e-19 8.14771643e-03 4.60426291e-39]
 [6.47562560e-02 1.73259604e-05 2.12112202e-02 5.05695235e-02
  6.67813143e-02 5.46638241e-10 3.43145283e-02 2.29783808e-02
  7.06007667e-02 1.83269804e-12 2.26714128e-03 5.02886564e-03
  7.28051731e-02 3.72349136e-02 1.14817257e-01 1.92319303e-06
  3.28946287e-02 1.53704494e-03 1.00830148e-01 1.07044308e-01
  1.32110256e-01 5.77443034e-07 1.63311025e-02 2.14417769e-02
  4.53428257e-03 1.87385009e-02 1.15278370e-03]]

initial states:
[[1.00000000e+00 1.45406913e-11]]
#+end_example

* 200 iterations on brown
#+RESULTS:
#+begin_example
27 [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
50000 out of 50000
1.0000299999999998
1.00003
the model never stopped improving
CPU times: user 3min 51s, sys: 345 ms, total: 3min 51s
Wall time: 3min 52s
transition:
[[0.23221903 0.76778097]
 [0.70229989 0.29770011]]

observation:
[[2.81739822e-001 1.43132488e-001 8.48434970e-039 9.75851831e-005
  2.69362672e-015 2.26567133e-001 1.96209466e-052 3.91264554e-003
  3.99838821e-004 1.28228959e-001 5.84694599e-089 2.88371026e-003
  4.14028555e-011 8.80582911e-042 6.35909627e-035 1.38234418e-001
  1.36580291e-003 1.62526402e-074 2.92938439e-044 4.11414990e-009
  1.87721887e-002 4.67200518e-002 8.31615790e-092 2.25223167e-053
  6.94701181e-054 7.94535098e-003 3.77091958e-115]
 [6.45306779e-002 2.28885541e-011 2.11388638e-002 5.09197312e-002
  6.65567850e-002 3.53676378e-028 3.41974735e-002 2.33806094e-002
  7.04417813e-002 4.39068533e-035 2.25940754e-003 5.05942349e-003
  7.26074017e-002 3.71078968e-002 1.14425589e-001 7.88275501e-014
  3.32161677e-002 1.53180173e-003 1.00486193e-001 1.06804872e-001
  1.33136144e-001 1.33386835e-016 1.62753933e-002 2.13686341e-002
  4.51881509e-003 1.88874875e-002 1.14885129e-003]]

initial states:
[[1.00000000e+00 2.47001026e-27]]
#+end_example

* rand res
#+begin_example
27 [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
50000 out of 50000
the model stopped improving at iteration 174
CPU times: user 3min 28s, sys: 216 ms, total: 3min 28s
Wall time: 3min 28s
transition:
[[0.2974586  0.7025414 ]
 [0.76846507 0.23153493]]

observation:
[[6.55124847e-02 5.71952340e-11 2.11330838e-02 5.08030870e-02
  6.65385862e-02 3.93175379e-25 3.41881229e-02 2.32168465e-02
  7.07175312e-02 2.43068346e-26 2.25878976e-03 5.05000573e-03
  7.25875474e-02 3.70977504e-02 1.14394302e-01 4.80742210e-13
  3.31094650e-02 1.53138289e-03 1.00458717e-01 1.06774963e-01
  1.32618810e-01 4.32678209e-12 1.62709432e-02 2.13627913e-02
  4.51757952e-03 1.87086727e-02 1.14853716e-03]
 [2.80742624e-01 1.43181293e-01 2.26272409e-29 2.09981536e-04
  2.64622305e-10 2.26602508e-01 1.24335623e-37 4.08612102e-03
  7.72735123e-05 1.28272682e-01 2.02411595e-67 2.89348203e-03
  1.59773171e-09 3.42954114e-32 6.43777215e-29 1.38281553e-01
  1.47305209e-03 1.26206542e-55 7.16990137e-37 7.74877804e-07
  1.93046624e-02 4.67359820e-02 2.40155627e-72 2.12006626e-37
  1.26711428e-45 8.13801016e-03 1.70507529e-93]]

initial states:
[[0.47197308 0.52802692]]
#+end_example

* Questions
 - "For example, the DP solution must have valid state transitions" ? How can transitions be invalid ?
 - Where does the initial states distribution matrix come from ?
 - Does $N \times M$ means $N$ rows $M$ columns or $N$ columns $M$ rows

